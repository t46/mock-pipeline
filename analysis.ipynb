{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from langchain import PromptTemplate\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please output multiple specific hypotheses in list format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses = \"\"\"\n",
    "Hypothesis 1:\n",
    "The LLM is programmed to provide a more conversational response, which includes additional context or framing. If we modify the programming to focus on direct responses, the LLM will output only the direct answer to the question.\n",
    "\n",
    "Hypothesis 2:\n",
    "The LLM's responses are influenced by the style of the input prompt. If we adjust the prompt to be more direct and less conversational, the LLM will respond with a more direct answer.\n",
    "\n",
    "Hypothesis 3:\n",
    "The LLM is trained on a dataset that includes conversational responses. If we train the LLM on a dataset that includes only direct responses, the LLM will output more direct answers.\n",
    "\n",
    "Hypothesis 4:\n",
    "The LLM's tendency to provide additional context is a result of its attempt to ensure clarity and avoid ambiguity. If we provide the LLM with more specific instructions about the desired format of the response, it will output more direct answers.\n",
    "\n",
    "Hypothesis 5:\n",
    "The LLM's responses are influenced by the complexity of the question. If we simplify the questions, the LLM will provide more direct answers.\n",
    "\n",
    "Hypothesis 6:\n",
    "The LLM's responses are influenced by the length of the input prompt. If we shorten the prompt, the LLM will provide a more direct answer.\n",
    "\n",
    "Hypothesis 7:\n",
    "The LLM's responses are influenced by the type of question asked. If we ask questions that require a numerical or one-word answer, the LLM will provide a more direct answer.\n",
    "\"\"\"\n",
    "\n",
    "template = \"\"\"\n",
    "Select the hypothesis that is easiest to test from among these hypotheses.\n",
    "-------------------------------------------------------------------\n",
    "{hypotheses}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"hypotheses\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_text = prompt.format(hypotheses=hypotheses)\n",
    "llm = OpenAI(model_name=\"gpt-4\", temperature=0.9)\n",
    "hypothesis = llm(prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hypothesis 2: The LLM's responses are influenced by the style of the input prompt. If we adjust the prompt to be more direct and less conversational, the LLM will respond with a more direct answer.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "The problem to solve is this:\n",
    "{problem}\n",
    "\n",
    "The hypothesis for this problem is this:\n",
    "{hypothesis}\n",
    "\n",
    "Given them, please make the hypothesis more specific and concrete with an example.\n",
    "\"\"\"\n",
    "\n",
    "problem = \"\"\"\n",
    "Background:\n",
    "We use a Large Language Model (LLM), which takes any text as input and outputs some text. We input instructions to the LLM in text, which is called prompt, to the LLM, and the LLM outputs text in response to those instructions.\n",
    "\n",
    "Problem:\n",
    "The problem is that the language model may also output sentences that are not directly related to the instructions. \n",
    "For example, suppose you enter the sentence \"What is 1 + 1?\" into the LLM. \n",
    "At this point, the LLM will often respond, \"The answer is 2\". In this answer statement, what we really want is the \"2\" part. The sentence \"The answer is\" is an extra sentence, and we want the LLM to output only the part that directly answers the question, not the extra sentence.\n",
    "The reason this is problematic is that the we must do some post-processing to evaluate the output sentences. For example, let's say you want to evaluate LLM's performance on a dataset of math problems. Suppose a sample is a question \"What is 1 + 1?\" paired with the correct answer \"2\". To evaluate the LLM's performance on this, we must examine whether the LLM's answer matches the correct answer. At this point, if the LLM outputs an extra sentence other than \"2,\" even if the answer is actually correct, it may be judged incorrect because of the apparent mismatch.\n",
    "However, if you try to address this in post-processing, you will have to depend on the problem. Also, it is difficult to deal with this in a predefined post-processing method, since it is not known in advance what kind of text will be output in areas that are not directly related to the question.\n",
    "\"\"\"\n",
    "\n",
    "hypothesis = \"\"\"\n",
    "The LLM's responses are influenced by the style of the input prompt. If we adjust the prompt to be more direct and less conversational, the LLM will respond with a more direct answer.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"problem\", \"hypothesis\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypothesis:\n",
      "\n",
      "The LLM's output structure and verbosity are affected by the wording and presentation of the input prompt. By adjusting the prompt in a more concise, non-conversational manner, the LLM may generate more direct and brief responses.\n",
      "\n",
      "For example, instead of using the prompt 'What is 1 + 1?', which is a common way of asking in daily language but might lead to a longer answer like 'The answer is 2', we can change the prompt to a more straightforward form such as 'Calculate 1 + 1'. In theory, this should encourage the LLM to respond specifically with '2', hence reducing the need for extensive post-processing for evaluation purposes. This approach will need to be tested across a wide range of queries to ascertain its effectiveness.\n"
     ]
    }
   ],
   "source": [
    "prompt_text = prompt.format(problem=problem, hypothesis=hypothesis)\n",
    "llm = OpenAI(model_name=\"gpt-4\", temperature=0.9)\n",
    "hypothesis = llm(prompt_text)\n",
    "print(hypothesis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autores",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

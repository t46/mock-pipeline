Background:
We use a Large Language Model (LLM), specifically GPT-4, which takes any text as input and outputs text in response. We input instructions, called prompts, to the LLM, and the LLM generates text based on those instructions.

Problem:
The issue is that the large language model may output sentences not directly related to the instructions. 
For example, if you enter the sentence "What is 1 + 1?" into the LLM, it will often respond with "The answer to that question is 2." In this response, what we really want is just the "2" part. The sentence "The answer to that question is" is extraneous, and we would prefer the LLM to output only the part that directly related to the question.

The reason this is problematic is that we must perform post-processing to evaluate the output. For instance, if you want to evaluate the LLM's performance on a dataset of math problems, and a sample is a question "What is 1 + 1?" paired with the correct answer "2", we must check whether the LLM's answer matches "2". If the LLM outputs an extra sentence besides "2," even if the answer is actually correct, it may be judged as incorrect due to the apparent mismatch.
It is challenging to address this issue with a predefined post-processing method, as it is not known in advance what kind of extraneous text will be output.

To sum up, the problems are as follows:
- The large language model outputs sentences that are not directly related to the instructions.
- Predefined post-processing methods are problem/answer-specific and not general.
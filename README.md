# mock-pipeline
## Repository Structure
```
.
├── README.md
├── input_data ...................................... Any data to be entered into the module should be placed here.
│   └── problem.txt ................................. A sample research problem read by the hypothesis_generation module.
├── modules ......................................... The modules that make up the pipeline.
│   ├── __init__.py
│   ├── hypothesis_generation.py .................... A module that generates a hypothesis given a problem.
│   ├── verification_design.py ...................... A module that designs a verification plan given problem and hypothesis.
│   ├── verification_execution.py ................... A module that executes a insntantiated verification plan.
│   └── verification_instantiation.py ............... A module that instantiates a verification plan given a design.
├── pipeline.py ..................................... The main pipeline script.
├── scripts ......................................... Scripts that are generated by the modules.
│   ├── package_install.py .......................... A script by verification_instantiation that installs required packages.
│   └── verification.py ............................. A script by verification_instantiation that executes the verification.
└── outputs ......................................... Generated outputs.
    ├── evaluation.csv .............................. The results of evaluation for each run.
    ├── 2023-09-07_15-04-15 ......................... Outputs saved at 2023-09-07_15-04-15
    │   ├── hypothesis_candidates.txt ............... Hypothesis candidates generated by "Hypothesis Candidate Generation".
    │   ├── hypothesis.txt .......................... Hypothesis generated (selected) by "Hypothesis Selection".
    │   ├── representation_of_hypothesis.txt ........ Reformulated hypothesis generated by "Hypothesis Reformulation".
    │   ├── verification_plan.txt ................... Verification plan generated by "Verification Plan Design".
    │   ├── raw_verification_code_in_text.txt ....... Verification code generated by "Verification Code Generation".
    │   ├── refined_verification_code_in_text.txt ... Verification code re-generated to follow instruction by "Instruction Following".
    │   ├── verification_code.py .................... Verification code extracted from refined_verification_code_in_text.txt. and saved as Python script.
    │   ├── verification_code_updated.py ............ Verification code updated based on error messages.
    │   └── package_install_code.py ................. Package installation code generated by "Package Install Code Generation".
    ├── 2023-09-07_15-04-16 ......................... Outputs saved at 2023-09-07_15-04-16
    │   ├── hypothesis_candidates.txt ............... Hypothesis candidates generated by "Hypothesis Candidate Generation".
```

## Quick Start
### Create a Virtual Environment
```shell
conda create -n autores
conda activate autores
conda install -c conda-forge langchain
```
### Set Environment Variables
```shell
export OPENAI_API_KEY="your-api-key"
```

### Run Pipeline
```shell
python pipeline.py
```

### Run Pipeline 50 Times
```shell
```shell
./run_pipeline.sh
```

## Where to Change
### New Branch
```shell
git checkout -b new-branch-name
```
### Prompt
The `template` in each module corresponds to the instructions to the language model, so change this part.

e.g. `modules/hypothesis_generation.py`
```python
import logging
from langchain import PromptTemplate

hypothesis_candidates_template = """
How can we solve the problem described below? Please provide multiple hypotheses in list format.

Problem:
{problem}
"""

hypothesis_selection_template = """
Please select the easiest-to-test hypothesis from among the hypotheses below.

Hypotheses:
{hypotheses}
"""
```

### Input Data
The input data is placed in the `input_data` directory. So, change them.

e.g. `test/sample-data/problem.txt`
```text
The research problem identified in the text is the need for manual post-processing of the output generated by large language models (LLM) when "Think step-by-step" is added in the prompt. More specifically, this issue emerges when evaluating mathematical tasks, as the output, such as "The answer is 1", must be manually processed to extract the numeric answer "1". The researchers highlight this as an area that needs to be addressed in future studies.
```

Then, run the module to get the result.
```shell
python test/test_hypothesis_generation.py
```
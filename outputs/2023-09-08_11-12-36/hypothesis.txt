Hypothesis: Refining the Prompting Strategy
   - We could refine the way we prompt the LLM. For instance, instead of asking "What is 1 + 1?", we could ask "Provide a one-word answer: What is 1 + 1?". This might encourage the model to generate more concise responses.
1. Refine the Prompt: One way to address this issue could be to refine the prompt given to the LLM. For example, instead of asking "What is 1 + 1?", the prompt could be "Provide a one-word answer: What is 1 + 1?" This might encourage the model to respond with just "2".

2. Post-Processing with NLP: Implement a Natural Language Processing (NLP) post-processing system that can extract the relevant information from the LLM's output. This system could be trained to identify and extract the answer from the output, regardless of the extraneous text.

3. Fine-Tuning the Model: Fine-tune the LLM on a dataset where the correct responses are only the direct answers. This could help the model learn to generate responses that are more directly related to the prompts.

4. Implement a Feedback Loop: Implement a feedback loop where the model's output is evaluated, and the feedback is used to adjust the model's future responses. This could help the model learn over time to provide more direct answers.

5. Use a Different Model: If GPT-4 is consistently providing extraneous information, it might be worth exploring other models that are more suited to providing direct answers.

6. Use a Wrapper Model: Implement a secondary "wrapper" model that takes the output of the LLM and processes it to extract the relevant information. This model could be trained specifically to handle the extraneous information output by the LLM.

7. Modify the Evaluation Method: Instead of checking for an exact match between the LLM's output and the correct answer, the evaluation method could be modified to check for the presence of the correct answer in the LLM's output. This would allow for correct evaluations even if the LLM outputs additional text.
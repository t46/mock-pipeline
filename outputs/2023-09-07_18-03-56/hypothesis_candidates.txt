1. Refine the Prompt: One way to address this issue could be to refine the prompt given to the LLM. For example, instead of asking "What is 1 + 1?", the prompt could be "Provide the numerical answer to 1 + 1". This might encourage the model to provide a more direct response.

2. Post-Processing with NLP: Implement a Natural Language Processing (NLP) post-processing system that can extract the relevant information from the LLM's output. This could involve named entity recognition, part-of-speech tagging, or other NLP techniques to identify and extract the answer.

3. Training with Direct Answer Dataset: Train the LLM with a dataset that includes direct answers to questions. This could help the model learn to provide more direct responses.

4. Implementing a Feedback Loop: Implement a feedback loop where the LLM's responses are evaluated and the feedback is used to improve future responses. This could involve reinforcement learning techniques.

5. Using a Wrapper Model: Use a secondary, smaller model as a 'wrapper' around the LLM. This wrapper model could be trained to take the output of the LLM and refine it into a more direct answer.

6. Customizing the LLM: Customize the LLM to include a parameter that controls the verbosity of the output. This could allow users to request more direct responses when needed.

7. Using Answer Extraction Techniques: Implement answer extraction techniques used in Question Answering (QA) systems. These techniques are designed to extract the most relevant answer from a larger text.

8. Implementing a Ranking System: Implement a ranking system that ranks different parts of the LLM's output based on their relevance to the input prompt. The highest-ranked part could then be selected as the final output.